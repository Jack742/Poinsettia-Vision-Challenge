# -*- coding: utf-8 -*-
"""Poinsettia Object Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OahKA8jAUoeIujX8xgFoHjEcWKHOzcAG
"""

import os
import json
import re
import numpy as np
from PIL import Image
import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import random
import torch
import math
from torchvision.transforms import functional as F
import sys



root_path = ''
test_data_dir = 'test'

def collate_fn(batch):
  """Taken from https://github.com/pytorch/vision/blob/master/references/detection/utils.py
  Allows PIL Images to be used in place of Tensors
  """
  
  return tuple(zip(*batch))


def _flip_coco_person_keypoints(kps, width):
    """https://github.com/pytorch/vision/blob/master/references/detection/transforms.py"""
    flip_inds = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]
    flipped_data = kps[:, flip_inds]
    flipped_data[..., 0] = width - flipped_data[..., 0]
    # Maintain COCO convention that if visibility == 0, then x, y = 0
    inds = flipped_data[..., 2] == 0
    flipped_data[inds] = 0
    return flipped_data


class Compose(object):
  """https://github.com/pytorch/vision/blob/master/references/detection/transforms.py"""
  def __init__(self, transforms):
        self.transforms = transforms

  def __call__(self, image, target):
        for t in self.transforms:
            image, target = t(image, target)
        return image, target


class RandomHorizontalFlip(object):
    """https://github.com/pytorch/vision/blob/master/references/detection/transforms.py"""
    def __init__(self, prob):
        self.prob = prob

    def __call__(self, image, target):
        if random.random() < self.prob:
            height, width = image.shape[-2:]
            image = image.flip(-1)
            bbox = target["boxes"]
            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]
            target["boxes"] = bbox
            if "masks" in target:
                target["masks"] = target["masks"].flip(-1)
            if "keypoints" in target:
                keypoints = target["keypoints"]
                keypoints = _flip_coco_person_keypoints(keypoints, width)
                target["keypoints"] = keypoints
        return image, target


class ToTensor(object):
    def __call__(self, image, target):
        image = F.to_tensor(image)
        return image, target


class PoinsettiaDataset(object):
  """
  Custom Dataloader for the Poinsettia dataset
  Loads image, parses labelling data, applies image augmentation
  """
  def __init__(self, root, img_folder, transforms=None,train=True):
    self.root = root
    self.train = train
    self.transforms = transforms
    self.img_folder = img_folder
    self.imgs = list(sorted(os.listdir(os.path.join(root, img_folder))))


  def __getitem__(self, idx):
    #Returns specified image and its target info    
    #Load Image    
    img_path = os.path.join(self.root,self.img_folder,self.imgs[idx])
    img = Image.open(img_path).convert("RGB")
    #Parse targets data
    target = {}
    box_tensor = []
    area_tensor = []
    iscrowd_tensor = []
    label_tensor = []
    img_id =  int(re.search(r'\d+', self.imgs[idx]).group())
    if self.train:
      #Get Target data
      with open(os.path.join(self.root,'poinsettia_train.json'), 'rb') as f:
        targets_data = json.load(f)

      
      for each in targets_data["annotations"]:
          if each["image_id"] == img_id:
            box = each["bbox"]
            xmin,ymin = int(box[0]), int(box[1])      
            xmax = xmin + int(box[2])
            ymax = ymin + int(box[3])
            box_tensor.append([xmin,ymin,xmax,ymax])
            area_tensor.append(each['area'])
            iscrowd_tensor.append(each['iscrowd'])
            #Only 1 class
            label_tensor.append(1)

    target['boxes'] = torch.Tensor(box_tensor)
    target['image_id'] = torch.Tensor([int(img_id) for _ in range(len(self.imgs))])
    target['labels'] = torch.Tensor(label_tensor)
    target['labels'] = target['labels'].type(torch.int64)
    target['iscrowd'] = torch.Tensor(iscrowd_tensor)
    target['masks'] = self.create_mask(img, target['boxes']) if self.train else torch.Tensor([])

    #Apply Transformations
    if self.transforms is not None:
      img, target = self.transforms(img, target)
    return img,target

  
  def __len__(self):
    return len(self.imgs)
    
  def create_mask(self, img, boxes):
    #Creates a mask of the image
    rows,cols = img.size
    masks = []
    for box in boxes:
      boxed_mask = np.zeros((rows,cols))
      xmin,ymin = int(box[0].item()), int(box[1].item())      
      xmax = int(box[2].item())
      ymax = int(box[3].item())
      boxed_mask[xmin:xmax, ymin:ymax] = 1
      masks.append(boxed_mask)
    masks = torch.Tensor(masks)
    return masks


def get_model_instance_segmentation(num_classes):
    # load an instance segmentation model pre-trained pre-trained on COCO
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)

    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    # now get the number of input features for the mask classifier
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    # and replace the mask predictor with a new one
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,
                                                       hidden_layer,
                                                       num_classes)
    return model

def get_transforms(train):
  transforms = []
  transforms.append(ToTensor())
  if train:
    transforms.append(RandomHorizontalFlip(0.5))
  return Compose(transforms)

def main(mode):
  device = torch.device('cuda')   if torch.cuda.is_available() else torch.device('cpu')
  num_classes = 2
  dataset = PoinsettiaDataset(root_path, 'train', get_transforms(True))
  #Change this path to unseen data if you want to get output for new images
  dataset_test = PoinsettiaDataset(root_path, test_data_dir, get_transforms(False), train=False)
  dataloader = torch.utils.data.DataLoader(dataset, batch_size=2,shuffle=True,num_workers=4,collate_fn=collate_fn)
  dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1,shuffle=False,num_workers=4,collate_fn=collate_fn)
  model = get_model_instance_segmentation(num_classes)
  model.to(device)
  if mode=='train':  
    params = [p for p in model.parameters() if p.requires_grad]
    optimiser = torch.optim.SGD(params, lr=0.005, momentum=0.9,weight_decay=0.0005)

    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=3,gamma=0.1)
    num_epochs = 25

    mean_train_loss_per_epoch = []

    for epoch in range(num_epochs):
      model.train()
      loss_ = 0
      length = 0
      for i, data in enumerate(dataloader):
        images, targets = data[0], data[1]
        images = list(image.to(device) for image in images)
        targets = [{k:v.to(device) for k, v in t.items()} for t in targets]
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        loss_value = losses.item()
        loss_ += loss_value
        length += 1
        if not math.isfinite(loss_value):
          print("loss is {}, stopping training".format(loss_value))
          sys.exit(1)
        optimiser.zero_grad()
        losses.backward()
        optimiser.step()
      print("Epoch %d : Training Loss: %.3f" % (epoch,loss_/length))
      torch.save(model, os.path.join(root_path, "poinsettia_frcnn" + str(epoch)))
      torch.save(model.state_dict(), os.path.join(root_path, "poinsettia_frcnn_state_dict" + str(epoch)))
  elif mode=='test':
    model.load_state_dict(torch.load(os.path.join(root_path, "poinsettia_frcnn_state_dict19")))
    
  with torch.no_grad():
    model.eval()
    predictions = []
    id = 0 
    for j, test_data in enumerate(dataloader_test):
      images, targets = test_data[0], test_data[1]
      images = list(image.to(device) for image in images)
      targets = [{k:v.to(device) for k, v in t.items()} for t in targets]
      outputs = model(images)
      fig,ax = plt.subplots(1)
      imgs = images[0].detach().cpu()
      imgs = np.array(imgs)
      imgs = np.transpose(imgs, (2, 1, 0))
      ax.imshow(imgs)
      out = outputs
      imgid = int(targets[0]['image_id'][0].item())
      for i,each in enumerate(out[0]['boxes']):
        each = each.detach().cpu()
        width = int(each[2]-each[0])
        height = int(each[3]-each[1])
        pred = {"id":id,"image_id":imgid, "category_id":1, "bbox":[int(each[0].item()), int(each[1].item()), width, height], "area":float(width*height), "iscrowd":0}
        predictions.append(pred)
        id += 1
  outp = {"annotations":predictions}
  print(outp)
  with open(os.path.join(root_path,'output.json'), 'w') as f:
    json.dump(outp, f)    

main('train')

